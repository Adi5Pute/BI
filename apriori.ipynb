{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0jNFAW_lm2L",
        "outputId": "96bdfa25-b2d7-4b41-87c7-494138e92b1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frequent Itemsets (min_support = 0.3):\n",
            "\n",
            "Level 1 Frequent Itemsets:\n",
            "Itemset: [1], Support Count: 4\n",
            "Itemset: [18], Support Count: 5\n",
            "Itemset: [0], Support Count: 6\n",
            "Itemset: [17], Support Count: 3\n",
            "\n",
            "Level 2 Frequent Itemsets:\n",
            "Itemset: [0, 18], Support Count: 3\n",
            "\n",
            "Association Rules (min_confidence = 0.6):\n",
            "If {18} → Then {0} (Confidence = 0.60)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load CSV and Preprocess (ignore Name, use Age, Gender, Marks)\n",
        "def load_and_preprocess_data(file_path):\n",
        "    transactions = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines[1:]:  # Skip header\n",
        "            values = line.strip().split(',')\n",
        "            age = int(values[1])\n",
        "            gender = 0 if values[2] == 'M' else 1  # M=0, F=1\n",
        "            marks = int(values[3])\n",
        "            transactions.append(set([age, gender, marks]))\n",
        "    return transactions\n",
        "\n",
        "# Step 2: Generate 1-itemsets\n",
        "def generate_1_itemsets(transactions):\n",
        "    itemsets = {}\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            if item in itemsets:\n",
        "                itemsets[item] += 1\n",
        "            else:\n",
        "                itemsets[item] = 1\n",
        "    return {frozenset([item]): count for item, count in itemsets.items()}\n",
        "\n",
        "# Step 3: Generate k-itemsets from previous frequent itemsets\n",
        "def generate_k_itemsets(prev_itemsets, k):\n",
        "    keys = list(prev_itemsets.keys())\n",
        "    candidates = {}\n",
        "    for i in range(len(keys)):\n",
        "        for j in range(i + 1, len(keys)):\n",
        "            union = keys[i].union(keys[j])\n",
        "            if len(union) == k:\n",
        "                candidates[union] = 0\n",
        "    return candidates\n",
        "\n",
        "# Step 4: Count support of candidates\n",
        "def count_support(candidates, transactions):\n",
        "    for transaction in transactions:\n",
        "        for itemset in candidates:\n",
        "            if itemset.issubset(transaction):\n",
        "                candidates[itemset] += 1\n",
        "    return candidates\n",
        "\n",
        "# Step 5: Filter itemsets by support\n",
        "def filter_by_support(itemsets, transactions, min_support):\n",
        "    frequent = {}\n",
        "    total = len(transactions)\n",
        "    for itemset, count in itemsets.items():\n",
        "        support = count / total\n",
        "        if support >= min_support:\n",
        "            frequent[itemset] = count\n",
        "    return frequent\n",
        "\n",
        "# Step 6: Generate association rules\n",
        "def generate_association_rules(all_frequent_itemsets, total_transactions, min_confidence):\n",
        "    rules = []\n",
        "    support_lookup = {}\n",
        "\n",
        "    # Build support lookup\n",
        "    for level in all_frequent_itemsets:\n",
        "        for itemset, count in level.items():\n",
        "            support_lookup[itemset] = count / total_transactions\n",
        "\n",
        "    for level in all_frequent_itemsets[1:]:  # skip 1-itemsets\n",
        "        for itemset in level:\n",
        "            for i in range(1, len(itemset)):\n",
        "                subsets = get_subsets(itemset, i)\n",
        "                for antecedent in subsets:\n",
        "                    consequent = itemset - antecedent\n",
        "                    if antecedent in support_lookup:\n",
        "                        confidence = support_lookup[itemset] / support_lookup[antecedent]\n",
        "                        if confidence >= min_confidence:\n",
        "                            rules.append((set(antecedent), set(consequent), confidence))\n",
        "    return rules\n",
        "\n",
        "# Helper: Get all subsets of given size\n",
        "def get_subsets(itemset, size):\n",
        "    from itertools import combinations\n",
        "    return [frozenset(s) for s in combinations(itemset, size)]\n",
        "\n",
        "# Step 7: Apriori main function\n",
        "def apriori(file_path, min_support, min_confidence):\n",
        "    transactions = load_and_preprocess_data(file_path)\n",
        "    all_frequent = []\n",
        "\n",
        "    one_itemsets = generate_1_itemsets(transactions)\n",
        "    frequent_itemsets = filter_by_support(one_itemsets, transactions, min_support)\n",
        "    all_frequent.append(frequent_itemsets)\n",
        "\n",
        "    k = 2\n",
        "    while True:\n",
        "        candidates = generate_k_itemsets(frequent_itemsets, k)\n",
        "        if not candidates:\n",
        "            break\n",
        "        candidates = count_support(candidates, transactions)\n",
        "        frequent_itemsets = filter_by_support(candidates, transactions, min_support)\n",
        "        if not frequent_itemsets:\n",
        "            break\n",
        "        all_frequent.append(frequent_itemsets)\n",
        "        k += 1\n",
        "\n",
        "    rules = generate_association_rules(all_frequent, len(transactions), min_confidence)\n",
        "    return all_frequent, rules\n",
        "\n",
        "# Step 8: Run and display results\n",
        "file_path = '/content/sample_data/sample_dataset(2).csv'  # Your uploaded file\n",
        "min_support = 0.3  # 30%\n",
        "min_confidence = 0.6  # 60%\n",
        "\n",
        "frequent_itemsets, association_rules = apriori(file_path, min_support, min_confidence)\n",
        "\n",
        "# Display frequent itemsets\n",
        "print(f\"\\nFrequent Itemsets (min_support = {min_support}):\")\n",
        "for i, level in enumerate(frequent_itemsets):\n",
        "    print(f\"\\nLevel {i + 1} Frequent Itemsets:\")\n",
        "    for itemset, count in level.items():\n",
        "        print(f\"Itemset: {list(itemset)}, Support Count: {count}\")\n",
        "\n",
        "# Display association rules\n",
        "print(f\"\\nAssociation Rules (min_confidence = {min_confidence}):\")\n",
        "for antecedent, consequent, confidence in association_rules:\n",
        "    print(f\"If {antecedent} → Then {consequent} (Confidence = {confidence:.2f})\")\n"
      ]
    }
  ]
}